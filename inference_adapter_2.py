import os
import json
import argparse
import torch
import numpy as np
from PIL import Image
from pathlib import Path
import warnings
# å½»åº•å…³é—­æ‰€æœ‰æ— å…³è­¦å‘Š
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=DeprecationWarning)

from diffusers import StableDiffusionPipeline

# ä»è®­ç»ƒè„šæœ¬å¯¼å…¥æ ¸å¿ƒç»„ä»¶å’Œè¶…å‚
from train_vehicle_adapter import (
    VehicleAdapter, AdapterAttentionProcessor,
    infer_unet_layer_dims, sanitize_key,
    D_id, P_FOURIER, D_obj, D_txt, H, type2idx
)

# --------------------------- å…¨å±€é…ç½®ï¼ˆä¸è®­ç»ƒè„šæœ¬ä¸¥æ ¼ä¸€è‡´ï¼‰ ---------------------------
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
IMG_SIZE = 256
C_TYPE = len(type2idx)
# è®­ç»ƒè„šæœ¬ç¡¬ç¼–ç çš„BATCH=2ï¼Œæ¨ç†æ—¶å¿…é¡»å¯¹é½
TRAIN_BATCH_SIZE = 2

# --------------------------- åŠ è½½é€‚é…å™¨æƒé‡ï¼ˆå…¼å®¹è®­ç»ƒçš„state_dictï¼‰ ---------------------------
def load_adapter_state(adapter, path, device):
    state = torch.load(path, map_location=device, weights_only=True)
    if isinstance(state, dict) and 'adapter_state_dict' in state:
        sd = state['adapter_state_dict']
    else:
        sd = state
    adapter.load_state_dict(sd, strict=False)
    adapter.to(device, dtype=torch.float32)
    adapter.eval()
    return adapter

# --------------------------- ä»JSONåŠ è½½ç‰¹å¾å¹¶æ„é€ B=2çš„è¾“å…¥ï¼ˆæ ¸å¿ƒä¿®å¤ï¼‰ ---------------------------
def load_and_pad_feat_to_batch2(json_path, device):
    """
    åŠ è½½ç‰¹å¾å¹¶å¤åˆ¶ä¸ºbatch size=2ï¼ŒåŒ¹é…è®­ç»ƒè„šæœ¬çš„ç¡¬ç¼–ç B=2
    è¿”å›ï¼šæ‰€æœ‰ç‰¹å¾çš„shapeå‡ä¸º[2, 1, D]æˆ–[2,4]ï¼ˆB=2ï¼‰
    """
    if not os.path.exists(json_path):
        raise FileNotFoundError(f"JSONç‰¹å¾æ–‡ä»¶ä¸å­˜åœ¨ï¼š{json_path}")
    with open(json_path, "r", encoding="utf-8") as f:
        json_data = json.load(f)
    
    # æå–å•ç›®æ ‡ç‰¹å¾
    first_image = json_data["image_results"][0]
    first_box = first_image["box_results"][0]
    
    # åŸºç¡€ç‰¹å¾ï¼ˆB=1ï¼‰
    id_emb_1 = torch.tensor(first_box["embeddings"], dtype=torch.float32, device=device).unsqueeze(0).unsqueeze(0)  # [1,1,768]
    box_coords_1 = torch.tensor(first_box["box"], dtype=torch.float32, device=device).unsqueeze(0)  # [1,4]
    type_onehot_1 = torch.tensor(first_box["anno"]["type_onehot"], dtype=torch.float32, device=device).unsqueeze(0).unsqueeze(0)  # [1,1,8]
    box_mask_1 = torch.tensor([[1.0]], dtype=torch.float32, device=device)  # [1,1]ï¼ˆ2ç»´ï¼‰
    
    # --------------------------- æ ¸å¿ƒä¿®å¤ï¼šå¤åˆ¶ç‰¹å¾æ„é€ B=2 ---------------------------
    id_emb = torch.cat([id_emb_1, id_emb_1], dim=0)  # [2,1,768]
    box_coords = torch.cat([box_coords_1, box_coords_1], dim=0)  # [2,4]
    type_onehot = torch.cat([type_onehot_1, type_onehot_1], dim=0)  # [2,1,8]
    box_mask = torch.cat([box_mask_1, box_mask_1], dim=0)  # [2,1]ï¼ˆæ€»å…ƒç´ æ•°=2ï¼ŒåŒ¹é…view(2,-1)ï¼‰
    # --------------------------------------------------------------------------------
    
    # æ‰“å°ç‰¹å¾ä¿¡æ¯
    print(f"âœ… ä»JSONåŠ è½½ç‰¹å¾å¹¶æ„é€ B=2æˆåŠŸ")
    print(f"  è½¦è¾†ç±»å‹ï¼š{first_box['anno']['type']} | å½’ä¸€åŒ–æ¡†åæ ‡ï¼š{np.round(first_box['box'], 4)}")
    print(f"  IDç‰¹å¾ç»´åº¦ï¼š{id_emb.shape} | Boxç»´åº¦ï¼š{box_coords.shape} | Maskç»´åº¦ï¼š{box_mask.shape}")
    return id_emb, box_coords, type_onehot, box_mask

# --------------------------- æ ¸å¿ƒæ¨ç†å‡½æ•° ---------------------------
def infer(adapter_path, json_feat_path, prompt, out_path, num_samples=1, device=DEVICE):
    # 1. åŠ è½½Stable Diffusion Pipelineï¼ˆä½ç‰ˆæœ¬å…¼å®¹ï¼‰
    print("ğŸ”§ åŠ è½½Stable Diffusionæ¨¡å‹...")
    pipe = StableDiffusionPipeline.from_pretrained(
        "runwayml/stable-diffusion-v1-5",
        safety_checker=None,
        requires_safety_checker=False
    )
    # ä»…å¯¹PyTorchæ¨¡å‹åšè®¾å¤‡/ç²¾åº¦è½¬æ¢
    pipe.unet = pipe.unet.to(device, dtype=torch.float32)
    pipe.vae = pipe.vae.to(device, dtype=torch.float32)
    pipe.text_encoder = pipe.text_encoder.to(device, dtype=torch.float32)
    # å…³é—­æ¢¯åº¦ + æ˜¾å­˜ä¼˜åŒ–
    pipe.unet.requires_grad_(False)
    pipe.vae.requires_grad_(False)
    pipe.text_encoder.requires_grad_(False)
    pipe.enable_vae_slicing()
    pipe.enable_attention_slicing()

    # 2. åˆå§‹åŒ–è½¦è¾†é€‚é…å™¨ï¼ˆä¸è®­ç»ƒä¸¥æ ¼ä¸€è‡´ï¼‰
    print("ğŸ”§ åˆå§‹åŒ–è½¦è¾†é€‚é…å™¨...")
    layer_dim_map = infer_unet_layer_dims(pipe.unet, default_heads=8)
    original_keys = list(pipe.unet.attn_processors.keys())
    layer_name_map = {orig: sanitize_key(orig) for orig in original_keys}
    safe_keys = [sanitize_key(orig) for orig in original_keys]
    # æ„å»ºå±‚ç»´åº¦å­—å…¸
    per_layer_dims = {}
    for orig_name, safe in layer_name_map.items():
        info = layer_dim_map.get(orig_name)
        if info is not None:
            per_layer_dims[safe] = {
                'heads': int(info['heads']),
                'd_h': int(info['d_h']),
                'hidden_size': int(info['hidden_size'])
            }
    # åˆ›å»ºé€‚é…å™¨
    adapter = VehicleAdapter(
        D_id=D_id,
        P_box=P_FOURIER,
        C_type=C_TYPE,
        D_obj=D_obj,
        layers=safe_keys,
        D_txt=D_txt,
        H=H,
        per_layer_dims=per_layer_dims
    )
    # åŠ è½½æƒé‡
    adapter = load_adapter_state(adapter, adapter_path, device)
    # è¡¥å…¨å±‚åç§°æ˜ å°„ï¼ˆè®­ç»ƒæ—¶çš„å…³é”®é…ç½®ï¼‰
    adapter.layer_name_map = layer_name_map

    # 3. æŒ‚è½½é€‚é…å™¨æ³¨æ„åŠ›å¤„ç†å™¨
    print("ğŸ”§ æŒ‚è½½é€‚é…å™¨æ³¨æ„åŠ›å¤„ç†å™¨...")
    new_processors = {}
    for layer_name in pipe.unet.attn_processors.keys():
        orig_proc = pipe.unet.attn_processors[layer_name]
        layer_info = layer_dim_map.get(layer_name, None)
        new_processors[layer_name] = AdapterAttentionProcessor(
            orig_proc, adapter, layer_name, layer_info
        ).to(device, dtype=torch.float32)
    pipe.unet.set_attn_processor(new_processors)

    # 4. åŠ è½½B=2çš„ç‰¹å¾å¹¶åˆå§‹åŒ–é€‚é…å™¨ç¼“å­˜ï¼ˆå…³é”®ï¼åŒ¹é…è®­ç»ƒç¡¬ç¼–ç ï¼‰
    print(f"ğŸ”§ åŠ è½½æ¨ç†ç‰¹å¾å¹¶æ„é€ B=2ï¼š{json_feat_path}")
    id_emb, box_coords, type_onehot, box_mask = load_and_pad_feat_to_batch2(json_feat_path, device)
    # é€‚é…å™¨å‰å‘ï¼šç”¨B=2çš„ç‰¹å¾åˆå§‹åŒ–KV/gate/maskç¼“å­˜
    with torch.no_grad():
        Ks, Vs, g, mask = adapter(id_emb, box_coords, type_onehot, box_mask)
    # ç¡®è®¤ç¼“å­˜åˆå§‹åŒ–æˆåŠŸ
    assert adapter.last_Ks is not None and adapter.last_Vs is not None, "é€‚é…å™¨ç¼“å­˜åˆå§‹åŒ–å¤±è´¥ï¼"

    # 5. å›¾åƒç”Ÿæˆï¼ˆç”Ÿæˆæ—¶ä»ç”¨å•æ ·æœ¬ï¼Œå–ç¼“å­˜çš„ç¬¬ä¸€ä¸ªæ ·æœ¬ï¼‰
    print(f"ğŸš€ å¼€å§‹ç”Ÿæˆå›¾åƒï¼šprompt='{prompt}'ï¼Œæ ·æœ¬æ•°={num_samples}")
    generator = torch.Generator(device=device).manual_seed(42)
    images = []
    for i in range(num_samples):
        with torch.no_grad():
            # ç”Ÿæˆæ—¶batch size=1ï¼Œé€‚é…å™¨ä¼šè‡ªåŠ¨ä½¿ç”¨ç¼“å­˜çš„å‰1ä¸ªæ ·æœ¬ç‰¹å¾
            img = pipe(
                prompt,
                num_inference_steps=30,
                guidance_scale=7.5,
                generator=generator,
                height=512,
                width=512
            ).images[0]
        images.append(img)
        print(f"âœ… ç”Ÿæˆå®Œæˆï¼šç¬¬{i+1}/{num_samples}å¼ ")

    # 6. ä¿å­˜å›¾åƒ
    os.makedirs(os.path.dirname(out_path), exist_ok=True)
    for i, img in enumerate(images):
        save_path = out_path if num_samples == 1 else out_path.replace('.png', f'_{i}.png')
        img.save(save_path)
        print(f"ğŸ’¾ å›¾åƒä¿å­˜ï¼š{save_path}")
    print(f"\nğŸ‰ æ¨ç†å®Œæˆï¼æ‰€æœ‰å›¾åƒå·²ä¿å­˜è‡³ {os.path.dirname(out_path)}")

# --------------------------- å‘½ä»¤è¡Œå‚æ•°è§£æ ---------------------------
def cli():
    parser = argparse.ArgumentParser(description="è½¦è¾†ç‰¹å¾å¼•å¯¼çš„SDæ¨ç†é€‚é…å™¨ï¼ˆå…¼å®¹è®­ç»ƒB=2ç¡¬ç¼–ç ï¼‰")
    # å¿…é€‰å‚æ•°
    parser.add_argument("--adapter", required=True, help="é€‚é…å™¨æƒé‡è·¯å¾„ï¼ˆå¦‚adapter_final.pthï¼‰")
    parser.add_argument("--json", required=True, help="æ¨ç†ç‰¹å¾JSONæ–‡ä»¶è·¯å¾„")
    # å¯é€‰å‚æ•°
    parser.add_argument("--prompt", default="a photo of a car, high resolution, realistic", help="ç”Ÿæˆæç¤ºè¯")
    parser.add_argument("--out", default="./output/gen_car.png", help="è¾“å‡ºå›¾åƒè·¯å¾„")
    parser.add_argument("--num_samples", type=int, default=1, help="ç”Ÿæˆæ ·æœ¬æ•°é‡ï¼ˆæ˜¾å­˜ä¸è¶³è¯·è®¾ä¸º1ï¼‰")
    parser.add_argument("--device", default="cuda", help="è¿è¡Œè®¾å¤‡ï¼ˆcuda/cpuï¼‰")
    
    args = parser.parse_args()
    # æ‰§è¡Œæ¨ç†
    infer(
        adapter_path=args.adapter,
        json_feat_path=args.json,
        prompt=args.prompt,
        out_path=args.out,
        num_samples=args.num_samples,
        device=args.device
    )


# ========== ç¨‹åºå…¥å£ï¼ˆæ— ä¿®æ”¹ï¼‰ ==========
if __name__ == '__main__':
    """
    python inference_adapter_2.py \
        --adapter /media/c303-2/225b3449-be93-436c-9dca-9188d2e145c21/cuiy/3dgsEnhancedDiffusion/TransReID/3drealcar_dataset/save_vehicle_adapter/adapter_final.pth \
        --json /media/c303-2/225b3449-be93-436c-9dca-9188d2e145c21/cuiy/3dgsEnhancedDiffusion/TransReID/3drealcar_dataset/processed_output/anns/2024_01_11_15_03_39.json \
        --prompt "a photo of a black SUV on the highway, 8k, realistic, sunny day" \
        --out ./output/black_suv.png \
        --num_samples 3 \
        --device cuda
    """
    cli()